{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "optical-wagon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/center/.pyenv/versions/3.6.12/envs/geo-search/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-24 20:28:41,530 loading file /home/center/.flair/models/de-ner-conll03-v0.4.pt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "import syntok.segmenter as segmenter\n",
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence\n",
    "\n",
    "flairTagger = SequenceTagger.load(\"de-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "requested-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Prepare\n",
    "# ### Load Spacy model\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# ### Read reports\n",
    "\n",
    "pol_df = pd.read_json(\"policereports.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "later-lawrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## Process\n",
    "# ### Tokenize with Spacy\n",
    "\n",
    "pol_df_extended = pol_df.copy()\n",
    "\n",
    "pol_df_extended[\"Token\"] = pol_df[\"Content\"].apply(lambda doc: nlp(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "herbal-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Named entity recognition\n",
    "# Only locations\n",
    "\n",
    "\n",
    "pol_df_extended[\"LocationsFromNER\"] = pol_df_extended[\"Token\"].apply(\n",
    "    lambda doc: {ent for ent in doc.ents if ent.label_ == \"LOC\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "violent-timothy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Sentence offsets\n",
    "\n",
    "pol_df_extended[\"SentenceOffsets\"] = pol_df_extended[\"Content\"].apply(\n",
    "    lambda content: np.asarray(\n",
    "        [list(sent)[0].offset for parag in segmenter.analyze(content) for sent in parag]\n",
    "        + [len(content)]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "disciplinary-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### NE offsets\n",
    "\n",
    "\n",
    "pol_df_extended[\"LocationsOffsets\"] = pol_df_extended[\"LocationsFromNER\"].apply(\n",
    "    lambda locations: np.asarray([location.start_char for location in locations])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "severe-electric",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Relevant sentence indices\n",
    "\n",
    "\n",
    "def getRelevantSentenceIndexes(row):\n",
    "    sentenceOffsets = row[0]\n",
    "    locationsOffsets = row[1]\n",
    "    return set((np.searchsorted(sentenceOffsets, locationsOffsets) - 1).clip(min=0))\n",
    "\n",
    "\n",
    "pol_df_extended[\"RelevantSentenceIndexes\"] = pol_df_extended[\n",
    "    [\"SentenceOffsets\", \"LocationsOffsets\"]\n",
    "].apply(getRelevantSentenceIndexes, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "scheduled-public",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Relevant sentences\n",
    "\n",
    "\n",
    "def pairwise(sequence):\n",
    "    return np.asarray(list(zip(sequence[:-1], sequence[1:])))\n",
    "\n",
    "\n",
    "def getRelevantSentences(row):\n",
    "    contentString = row[0]\n",
    "    sentenceOffsets = row[1]\n",
    "    relevantSentenceIndexes = row[2]\n",
    "    sentenceOffsetPairs = pairwise(sentenceOffsets)\n",
    "    relevantSentences = []\n",
    "    for idx in relevantSentenceIndexes:\n",
    "        start, end = sentenceOffsetPairs[idx]\n",
    "        relevantSentences.append(Sentence(contentString[start:end], use_tokenizer=True))\n",
    "    return relevantSentences\n",
    "\n",
    "\n",
    "pol_df_extended[\"RelevantSentences\"] = pol_df_extended[\n",
    "    [\"Content\", \"SentenceOffsets\", \"RelevantSentenceIndexes\"]\n",
    "].apply(getRelevantSentences, axis=1)\n",
    "\n",
    "\n",
    "def getLocationsFromFlair(relevantSentences):\n",
    "    predictions = flairTagger.predict(relevantSentences)\n",
    "    named_entities = []\n",
    "    for p in predictions:\n",
    "        current_spans = p.get_spans(\"ner\")\n",
    "        for span in current_spans:\n",
    "            if span.tag == \"LOC\":\n",
    "                named_entities.append(span.text)\n",
    "    return set(named_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "balanced-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_df.head().to_csv(\"testsample_policereports.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intense-budget",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Header</th>\n",
       "      <th>IsLocationInHeader</th>\n",
       "      <th>Title</th>\n",
       "      <th>URL</th>\n",
       "      <th>CreatedAt</th>\n",
       "      <th>Content</th>\n",
       "      <th>Token</th>\n",
       "      <th>LocationsFromNER</th>\n",
       "      <th>SentenceOffsets</th>\n",
       "      <th>LocationsOffsets</th>\n",
       "      <th>RelevantSentenceIndexes</th>\n",
       "      <th>RelevantSentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Polizeimeldung vom 31.12.2014 Neukölln</td>\n",
       "      <td>true</td>\n",
       "      <td>Geschossen, gebissen, geflüchtet</td>\n",
       "      <td>https://www.berlin.de/polizei/polizeimeldungen...</td>\n",
       "      <td>2018-11-24</td>\n",
       "      <td>Nr. 3099  Zu einem Überfall auf eine Spielothe...</td>\n",
       "      <td>(Nr., 3099,  , Zu, einem, Überfall, auf, eine,...</td>\n",
       "      <td>{(Britzer, Damm), (Neukölln), (Jahnstraße)}</td>\n",
       "      <td>[0, 93, 263, 356, 389, 490, 609, 664, 740]</td>\n",
       "      <td>[181, 83, 597]</td>\n",
       "      <td>{0, 1, 5}</td>\n",
       "      <td>[(Token: 1 Nr., Token: 2 3099, Token: 3 Zu, To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Polizeimeldung vom 31.12.2014 Reinickendorf</td>\n",
       "      <td>true</td>\n",
       "      <td>Seniorin beim Unfall schwer verletzt</td>\n",
       "      <td>https://www.berlin.de/polizei/polizeimeldungen...</td>\n",
       "      <td>2018-11-24</td>\n",
       "      <td>Nr. 3098  Ein Fußgängerin wurde gestern Nachmi...</td>\n",
       "      <td>(Nr., 3098,  , Ein, Fußgängerin, wurde, gester...</td>\n",
       "      <td>{(Hermsdorfer, Damm), (Hermsdorf), (Straße), (...</td>\n",
       "      <td>[0, 98, 286, 413, 492]</td>\n",
       "      <td>[174, 71, 267, 203]</td>\n",
       "      <td>{0, 1}</td>\n",
       "      <td>[(Token: 1 Nr., Token: 2 3098, Token: 3 Ein, T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Polizeimeldung vom 31.12.2014 Charlottenburg -...</td>\n",
       "      <td>true</td>\n",
       "      <td>Einbruch in Pfandleihhaus</td>\n",
       "      <td>https://www.berlin.de/polizei/polizeimeldungen...</td>\n",
       "      <td>2018-11-24</td>\n",
       "      <td>Nr. 3097  Unbekannte brachen heute früh in ein...</td>\n",
       "      <td>(Nr., 3097,  , Unbekannte, brachen, heute, frü...</td>\n",
       "      <td>{(Krummestraße), (Charlottenburg), (Einbrecher...</td>\n",
       "      <td>[0, 84, 318, 401, 485, 537, 616]</td>\n",
       "      <td>[471, 64, 511, 568, 285, 501, 262]</td>\n",
       "      <td>{0, 1, 3, 4, 5}</td>\n",
       "      <td>[(Token: 1 Nr., Token: 2 3097, Token: 3 Unbeka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Polizeimeldung vom 30.12.2014 Tempelhof - Schö...</td>\n",
       "      <td>true</td>\n",
       "      <td>Lokal überfallen</td>\n",
       "      <td>https://www.berlin.de/polizei/polizeimeldungen...</td>\n",
       "      <td>2018-11-24</td>\n",
       "      <td>Nr. 3094  Heute früh wurden Polizisten nach Te...</td>\n",
       "      <td>(Nr., 3094,  , Heute, früh, wurden, Polizisten...</td>\n",
       "      <td>{(Tempelhof), (Feurigstraße)}</td>\n",
       "      <td>[0, 95, 224, 304, 444, 482, 513, 589]</td>\n",
       "      <td>[44, 157]</td>\n",
       "      <td>{0, 1}</td>\n",
       "      <td>[(Token: 1 Nr., Token: 2 3094, Token: 3 Heute,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Polizeimeldung vom 30.12.2014 Mitte</td>\n",
       "      <td>true</td>\n",
       "      <td>960 Böller beschlagnahmt</td>\n",
       "      <td>https://www.berlin.de/polizei/polizeimeldungen...</td>\n",
       "      <td>2018-11-24</td>\n",
       "      <td>Nr. 3093  Zivilfahnder des Polizeiabschnitts 3...</td>\n",
       "      <td>(Nr., 3093,  , Zivilfahnder, des, Polizeiabsch...</td>\n",
       "      <td>{(Tüte), (Müller-, Ecke), (Wedding), (Lindower...</td>\n",
       "      <td>[0, 132, 291, 374, 417, 524, 570, 693]</td>\n",
       "      <td>[363, 171, 81, 184]</td>\n",
       "      <td>{0, 1, 2}</td>\n",
       "      <td>[(Token: 1 Nr., Token: 2 3093, Token: 3 Zivilf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Header IsLocationInHeader  \\\n",
       "0             Polizeimeldung vom 31.12.2014 Neukölln               true   \n",
       "1        Polizeimeldung vom 31.12.2014 Reinickendorf               true   \n",
       "2  Polizeimeldung vom 31.12.2014 Charlottenburg -...               true   \n",
       "3  Polizeimeldung vom 30.12.2014 Tempelhof - Schö...               true   \n",
       "4                Polizeimeldung vom 30.12.2014 Mitte               true   \n",
       "\n",
       "                                  Title  \\\n",
       "0      Geschossen, gebissen, geflüchtet   \n",
       "1  Seniorin beim Unfall schwer verletzt   \n",
       "2             Einbruch in Pfandleihhaus   \n",
       "3                      Lokal überfallen   \n",
       "4              960 Böller beschlagnahmt   \n",
       "\n",
       "                                                 URL   CreatedAt  \\\n",
       "0  https://www.berlin.de/polizei/polizeimeldungen...  2018-11-24   \n",
       "1  https://www.berlin.de/polizei/polizeimeldungen...  2018-11-24   \n",
       "2  https://www.berlin.de/polizei/polizeimeldungen...  2018-11-24   \n",
       "3  https://www.berlin.de/polizei/polizeimeldungen...  2018-11-24   \n",
       "4  https://www.berlin.de/polizei/polizeimeldungen...  2018-11-24   \n",
       "\n",
       "                                             Content  \\\n",
       "0  Nr. 3099  Zu einem Überfall auf eine Spielothe...   \n",
       "1  Nr. 3098  Ein Fußgängerin wurde gestern Nachmi...   \n",
       "2  Nr. 3097  Unbekannte brachen heute früh in ein...   \n",
       "3  Nr. 3094  Heute früh wurden Polizisten nach Te...   \n",
       "4  Nr. 3093  Zivilfahnder des Polizeiabschnitts 3...   \n",
       "\n",
       "                                               Token  \\\n",
       "0  (Nr., 3099,  , Zu, einem, Überfall, auf, eine,...   \n",
       "1  (Nr., 3098,  , Ein, Fußgängerin, wurde, gester...   \n",
       "2  (Nr., 3097,  , Unbekannte, brachen, heute, frü...   \n",
       "3  (Nr., 3094,  , Heute, früh, wurden, Polizisten...   \n",
       "4  (Nr., 3093,  , Zivilfahnder, des, Polizeiabsch...   \n",
       "\n",
       "                                    LocationsFromNER  \\\n",
       "0        {(Britzer, Damm), (Neukölln), (Jahnstraße)}   \n",
       "1  {(Hermsdorfer, Damm), (Hermsdorf), (Straße), (...   \n",
       "2  {(Krummestraße), (Charlottenburg), (Einbrecher...   \n",
       "3                      {(Tempelhof), (Feurigstraße)}   \n",
       "4  {(Tüte), (Müller-, Ecke), (Wedding), (Lindower...   \n",
       "\n",
       "                              SentenceOffsets  \\\n",
       "0  [0, 93, 263, 356, 389, 490, 609, 664, 740]   \n",
       "1                      [0, 98, 286, 413, 492]   \n",
       "2            [0, 84, 318, 401, 485, 537, 616]   \n",
       "3       [0, 95, 224, 304, 444, 482, 513, 589]   \n",
       "4      [0, 132, 291, 374, 417, 524, 570, 693]   \n",
       "\n",
       "                     LocationsOffsets RelevantSentenceIndexes  \\\n",
       "0                      [181, 83, 597]               {0, 1, 5}   \n",
       "1                 [174, 71, 267, 203]                  {0, 1}   \n",
       "2  [471, 64, 511, 568, 285, 501, 262]         {0, 1, 3, 4, 5}   \n",
       "3                           [44, 157]                  {0, 1}   \n",
       "4                 [363, 171, 81, 184]               {0, 1, 2}   \n",
       "\n",
       "                                   RelevantSentences  \n",
       "0  [(Token: 1 Nr., Token: 2 3099, Token: 3 Zu, To...  \n",
       "1  [(Token: 1 Nr., Token: 2 3098, Token: 3 Ein, T...  \n",
       "2  [(Token: 1 Nr., Token: 2 3097, Token: 3 Unbeka...  \n",
       "3  [(Token: 1 Nr., Token: 2 3094, Token: 3 Heute,...  \n",
       "4  [(Token: 1 Nr., Token: 2 3093, Token: 3 Zivilf...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_df_extended.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "successful-smile",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-afd2cbc473a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     pol_df_extended[\"LocationFromNERflair\"] = pol_df_extended[\"RelevantSentences\"][\n\u001b[1;32m      6\u001b[0m         \u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     ].apply(getLocationsFromFlair)\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mpol_df_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Output/output_{end}.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.12/envs/geo-search/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4211\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4212\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4213\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-4ea4ddada208>\u001b[0m in \u001b[0;36mgetLocationsFromFlair\u001b[0;34m(relevantSentences)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflairTagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevantSentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mnamed_entities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mcurrent_spans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_spans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mspan\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcurrent_spans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# ## Mini-Batch processing\n",
    "\n",
    "start = 400\n",
    "for end in range(500, pol_df.shape[0], 100):\n",
    "    pol_df_extended[\"LocationFromNERflair\"] = pol_df_extended[\"RelevantSentences\"][\n",
    "        start:end\n",
    "    ].apply(getLocationsFromFlair)\n",
    "    pol_df_extended.to_csv(f\"Output/output_{end}.csv\")\n",
    "    start = end\n",
    "\n",
    "end = pol_df.shape[0]\n",
    "pol_df_extended[\"LocationFromNERflair\"] = pol_df_extended[\"RelevantSentences\"][\n",
    "    start:end\n",
    "].apply(getLocationsFromFlair)\n",
    "pol_df_extended.to_csv(f\"Output/output_{end}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Result concatenation\n",
    "\n",
    "\n",
    "frames = []\n",
    "start = 0\n",
    "for end in range(100, pol_df.shape[0], 100):\n",
    "    current_frame = pd.read_csv(f\"Output/output_{end}.csv\")\n",
    "    frames.append(current_frame[:][start:end])\n",
    "    start = end\n",
    "\n",
    "end = pol_df.shape[0]\n",
    "current_frame = pd.read_csv(f\"Output/output_{end}.csv\")\n",
    "frames.append(current_frame[:][start:end])\n",
    "\n",
    "\n",
    "result = pd.concat(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-burton",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo-search",
   "language": "python",
   "name": "geo-search"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
